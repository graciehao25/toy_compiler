{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Source\n",
    "http://web.eecs.utk.edu/~azh/blog/teenytinycompiler1.html\n",
    "https://blog.usejournal.com/writing-your-own-programming-language-and-compiler-with-python-a468970ae6df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "    def __init__(self, input):\n",
    "        pass\n",
    "\n",
    "    # Process the next character.\n",
    "    def nextChar(self):\n",
    "        pass\n",
    "\n",
    "    # Return the lookahead character.\n",
    "    def peek(self):\n",
    "        pass\n",
    "\n",
    "    # Invalid token found, print error message and exit.\n",
    "    def abort(self, message):\n",
    "        pass\n",
    "\t\t\n",
    "    # Skip whitespace except newlines, which we will use to indicate the end of a statement.\n",
    "    def skipWhitespace(self):\n",
    "        pass\n",
    "\t\t\n",
    "    # Skip comments in the code.\n",
    "    def skipComment(self):\n",
    "        pass\n",
    "\n",
    "    # Return the next token.\n",
    "    def getToken(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token contains the original text and the type of token.\n",
    "class Token:   \n",
    "    def __init__(self, tokenText, tokenKind):\n",
    "        self.text = tokenText   # The token's actual text. Used for identifiers, strings, and numbers.\n",
    "        self.kind = tokenKind   # The TokenType that this token is classified as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TokenType is our enum for all the types of tokens.\n",
    "class TokenType(enum.Enum):\n",
    "\tEOF = -1\n",
    "\tNEWLINE = 0\n",
    "\tNUMBER = 1\n",
    "\tIDENT = 2\n",
    "\tSTRING = 3\n",
    "\t# Keywords.\n",
    "\tLABEL = 101\n",
    "\tGOTO = 102\n",
    "\tPRINT = 103\n",
    "\tINPUT = 104\n",
    "\tLET = 105\n",
    "\tIF = 106\n",
    "\tTHEN = 107\n",
    "\tENDIF = 108\n",
    "\tWHILE = 109\n",
    "\tREPEAT = 110\n",
    "\tENDWHILE = 111\n",
    "\t# Operators.\n",
    "\tEQ = 201  \n",
    "\tPLUS = 202\n",
    "\tMINUS = 203\n",
    "\tASTERISK = 204\n",
    "\tSLASH = 205\n",
    "\tEQEQ = 206\n",
    "\tNOTEQ = 207\n",
    "\tLT = 208\n",
    "\tLTEQ = 209\n",
    "\tGT = 210\n",
    "\tGTEQ = 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lex import Lexer, Token, TokenType\n",
    "\n",
    "\n",
    "def main():\n",
    "    input = \"IF+-123 foo*THEN/\"\n",
    "    lexer = Lexer(input)\n",
    "\n",
    "    token = lexer.getToken()\n",
    "    while token.kind != TokenType.EOF:\n",
    "        print(token.kind)\n",
    "        token = lexer.getToken()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
